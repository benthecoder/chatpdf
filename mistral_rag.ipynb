{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"eefc9fc15438408abd87de0bd4bfada8","deepnote_cell_type":"code"},"outputs":[],"source":["# %pip install pypdf mistralai faiss-cpu"]},{"cell_type":"markdown","metadata":{"cell_id":"f7a2b42e87a7448ab82859217a37c9fe","deepnote_cell_type":"markdown"},"source":["## Read pdf\n"]},{"cell_type":"code","execution_count":33,"metadata":{"cell_id":"d65d4ebe0bbb4ae2b76e9842fde78831","deepnote_cell_type":"code"},"outputs":[],"source":["from pathlib import Path\n","from pypdf import PdfReader\n","\n","\n","pdf_files = Path(\"data\").glob(\"*.pdf\")\n","text = \"\"\n","\n","for pdf_file in pdf_files:\n","    reader = PdfReader(pdf_file)\n","for page in reader.pages:\n","    text += page.extract_text() + \"\\n\\n\""]},{"cell_type":"code","execution_count":34,"metadata":{"cell_id":"4d3d0a7132f5459b9a2c4851b8a5946d","deepnote_cell_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["Benedict Neo\n","/envel‚å¢pebenedict.neo@outlook.com /linkedinin/benedictneo /github@benthecoder\n","Education\n","3817\n"]}],"source":["print(text[:100])\n","print(len(text))"]},{"cell_type":"markdown","metadata":{"cell_id":"b1a3ae57e28643e2b1381afc937cff42","deepnote_cell_type":"markdown"},"source":["## chunking\n"]},{"cell_type":"code","execution_count":54,"metadata":{"cell_id":"bb469994a3e04e7dabc94ed943dea42d","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["8"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["chunk_size = 500\n","chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n","len(chunks)"]},{"cell_type":"markdown","metadata":{"cell_id":"8949d65df6ea416ab2c726a63c5da763","deepnote_cell_type":"markdown"},"source":["in RAG, we need to split documents into smaller chunks so it's more effective to identify and retrieve teh most relevant information\n","\n","depending on the use case, a smaller chunk size will be beneficial for RAG to identify and extract relevant information more accurately, as larger text chunks can contain filler text that obscures the semantic representation\n","\n","Here, we combine 500 characters into one chunk, and we get 8 chunks.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"3ab474c2c9054088ac3d79f8d61038fc","deepnote_cell_type":"markdown"},"source":["## embed\n"]},{"cell_type":"markdown","metadata":{"cell_id":"7f3fd158f2cf42888d6d0c33fcb0d064","deepnote_cell_type":"markdown"},"source":["for each text chunk, we create a text embedding, which are numerical representations of text in the vector space.\n","\n","Words with similar meanings are closer to each other in this space.\n","\n","To create embeddings, we use Mistral AI's embeddings API endpoint.\n","\n","We create a simple embed function to get embeddings from a single chunk, store all of them in a numpy array\n"]},{"cell_type":"code","execution_count":55,"metadata":{"cell_id":"84b8cf81d0294033907fc53d16ba5ca0","deepnote_cell_type":"code"},"outputs":[],"source":["from mistralai.client import MistralClient\n","import numpy as np\n","\n","client = MistralClient(api_key=\"DP7QjbkK6AhscKlgjsL6qNgJs8fAIcUj\")\n","\n","\n","def embed(input: str):\n","    return client.embeddings(\"mistral-embed\", input=input).data[0].embedding\n","\n","\n","embeddings = np.array([embed(chunk) for chunk in chunks])\n","dimension = embeddings.shape[1]"]},{"cell_type":"code","execution_count":56,"metadata":{"cell_id":"1290cadf16c14c079affc05bf292501b","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["array([[-0.03314209,  0.03010559,  0.03341675, ..., -0.00093889,\n","         0.03649902,  0.01657104],\n","       [-0.03050232,  0.06610107,  0.06039429, ..., -0.02508545,\n","        -0.00403595, -0.02178955],\n","       [-0.02389526,  0.06365967,  0.04605103, ..., -0.02262878,\n","        -0.00494003, -0.02874756],\n","       ...,\n","       [-0.02015686,  0.03216553,  0.04882812, ..., -0.01455688,\n","         0.00720596, -0.0216217 ],\n","       [-0.00782013,  0.043396  ,  0.0413208 , ..., -0.02740479,\n","        -0.002491  , -0.02764893],\n","       [-0.0513916 ,  0.03872681,  0.03271484, ..., -0.01724243,\n","         0.03497314, -0.01080322]])"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["embeddings"]},{"cell_type":"code","execution_count":57,"metadata":{"cell_id":"d239780b30c445e4bbff726564d03918","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["1024"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["dimension"]},{"cell_type":"markdown","metadata":{"cell_id":"87bd2ecc25a244558bcf1dded914d91c","deepnote_cell_type":"markdown"},"source":["## vector db\n"]},{"cell_type":"markdown","metadata":{"cell_id":"f49ed9b2e7f34f8397425e184009dea2","deepnote_cell_type":"markdown"},"source":["once we have the embeddings, we store them in a vector db for efficient processing and retrieval.\n","\n","Here we use Faiss, an open source vector db developed by Meta.\n","\n","we create an index to store our embeddings.\n","\n","look at the different indexes: https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\n"]},{"cell_type":"code","execution_count":58,"metadata":{"cell_id":"2efccedce24a49a797803d9976b0ad82","deepnote_cell_type":"code"},"outputs":[],"source":["import faiss\n","\n","d = embeddings.shape[1]\n","index = faiss.IndexFlatL2(d)\n","index.add(embeddings)"]},{"cell_type":"markdown","metadata":{"cell_id":"e58f44e2b3a74045ad4dcbcca63666a2","deepnote_cell_type":"markdown"},"source":["## query\n"]},{"cell_type":"markdown","metadata":{"cell_id":"df6a7d452cb6483d8b3c17d403b4c239","deepnote_cell_type":"markdown"},"source":["when user asks a question, we create embeddings by using the same model as before.\n"]},{"cell_type":"code","execution_count":59,"metadata":{"cell_id":"2b0f04cf9b274cb3829be33e708df5fd","deepnote_cell_type":"code"},"outputs":[],"source":["question = \"Who is Benedict Neo?\"\n","question_embeddings = np.array([embed(question)])"]},{"cell_type":"markdown","metadata":{"cell_id":"7e5d64ffe0c447feba0960388ea79b42","deepnote_cell_type":"markdown"},"source":["## retrieval\n"]},{"cell_type":"markdown","metadata":{"cell_id":"2c3f24a85485425bb9014525f45b9a2e","deepnote_cell_type":"markdown"},"source":["we perform search on our vector db using `index.search`, it takes two parameters, the embedding of our question and k, which is the number of similar vectors to retrieve\n","\n","The function returns the distances (D) and indices (I) of the most similar vector, and based on the indices, we can return the actual text.\n"]},{"cell_type":"code","execution_count":60,"metadata":{"cell_id":"81d0a73a1c6b4832b5afe4ca0459a4c4","deepnote_cell_type":"code"},"outputs":[],"source":["D, I = index.search(question_embeddings, k=2)  # distance, index\n","retrieved_chunk = [chunks[i] for i in I.tolist()[0]]"]},{"cell_type":"markdown","metadata":{"cell_id":"3f8e92f27b2649c6aa9fb089243bff05","deepnote_cell_type":"markdown"},"source":["## Create prompt\n"]},{"cell_type":"markdown","metadata":{"cell_id":"e3d7f6718b554b1f9483b01d07a1e126","deepnote_cell_type":"markdown"},"source":["we create a prompt template that combines the chunk and the question\n"]},{"cell_type":"code","execution_count":61,"metadata":{"cell_id":"7fe48d0a6e674ad4b734991ee7890842","deepnote_cell_type":"code"},"outputs":[],"source":["prompt = f\"\"\"\n","Context information is below.\n","---------------------\n","{retrieved_chunk}\n","---------------------\n","Given the context information and not prior knowledge, answer the query.\n","Query: {question}\n","Answer:\n","\"\"\""]},{"cell_type":"markdown","metadata":{"cell_id":"5ca33d28cfb547029f8793eedf8390ff","deepnote_cell_type":"markdown"},"source":["## Chat model\n"]},{"cell_type":"markdown","metadata":{"cell_id":"69913c0ac100412a9341021eaec0ad19","deepnote_cell_type":"markdown"},"source":["using the mistral chat completion API with a mistral model, here we're using `mistral-medium`, we generate an answer based on the user question and the context retrieved\n"]},{"cell_type":"code","execution_count":62,"metadata":{"cell_id":"13a4ca45967f4191b5913ba2404e82fb","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["'Benedict Neo is a current student at Iowa State University pursuing a Bachelor of Science in Statistics with a minor in Computer Science, expected to graduate in December 2023. He has a GPA of 3.95. His coursework includes Experimental Design, Bayesian Statistics, Design & Analysis of Algorithms, and Large-scale Data Analysis.\\n\\nBenedict has worked as an Undergraduate Research Assistant at Iowa State University since January 2022. In this role, he led the development of the WEPPR R package, improved search speed by implementing concurrent processing in FastAPI, and built Svelte components to render search results in a user-friendly design.\\n\\nHis skills include programming languages such as Python, R, SQL, SAS, JavaScript (React), Java, HTML/CSS, and Bash. He is also proficient in libraries such as Pandas, NumPy, Matplotlib, Plotly, Tidyverse, Scikit-Learn, NLTK, PyTorch, and PySpark, and tools such as AWS, Google Cloud, Docker, Power BI, Tableau, Git, Linux, Hadoop, Spark, and Airflow.\\n\\nBenedict can be contacted via email at [envelope](mailto:benedict.neo@outlook.com)benedict.neo@outlook.com or LinkedIn at linkedin.com/in/benedictneo. He also has a GitHub profile at github.com/benthecoder.'"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["from mistralai.client import ChatMessage\n","\n","\n","def run_mistral(user_message, model=\"mistral-medium\"):\n","    messages = [ChatMessage(role=\"user\", content=user_message)]\n","    chat_response = client.chat(model=model, messages=messages)\n","    return chat_response.choices[0].message.content\n","\n","\n","run_mistral(prompt)"]},{"cell_type":"markdown","metadata":{"cell_id":"b6b388cdfc1948f094b2864433d01d62","deepnote_cell_type":"markdown"},"source":["## all together now\n"]},{"cell_type":"code","execution_count":63,"metadata":{"allow_embed":true,"cell_id":"06f000cec10342988f837b7f5e48d1e0","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["\"Based on the provided context information, the individual has experience as a President of the Google Developer Student Club at Iowa State University from August 2022 to May 2023. In this role, they led a team of 12 core officers to organize and host tech workshops and talks for over 100 students.\\n\\nAdditionally, they have experience as a Data Analyst Intern at Tesla from May 2022 to August 2022. Their responsibilities at Tesla included architecting and deploying Airflow ETL pipelines with Docker on Linux, designing and optimizing MySQL database schemas, developing Python packages interfacing with various APIs, and building an interactive web app with Streamlit for failure analysis using text embeddings and NLP techniques.\\n\\nIt's worth noting that the context information does not specify the duration of the individual's experience as a Data Analyst at a company with revenue up to $1 Million/month, but it can be inferred that they have experience with data visualization, hypothesis testing, regression, classification, NLP, and time series analysis based on the context.\""]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["from faiss import IndexFlatL2\n","\n","prompt = \"\"\"\n","Context information is below.\n","---------------------\n","{context}\n","---------------------\n","Given the context information and not prior knowledge, answer the query.\n","Query: {query}\n","Answer:\n","\"\"\"\n","\n","\n","def ask(query: str, index: IndexFlatL2, chunks):\n","    embedding = embed(query)\n","    embedding = np.array([embedding])\n","\n","    _, indexes = index.search(embedding, k=2)\n","    context = [chunks[i] for i in indexes.tolist()[0]]\n","\n","    user_message = prompt.format(context=context, query=query)\n","\n","    messages = [ChatMessage(role=\"user\", content=user_message)]\n","    chat_response = client.chat(model=\"mistral-medium\", messages=messages)\n","    return chat_response.choices[0].message.content\n","\n","\n","ask(\"What work experience does he have?\", index, chunks)"]},{"cell_type":"code","execution_count":64,"metadata":{"cell_id":"33cba9b079c7496ea6e859dcc1b9d185","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["'Yes, the individual listed Python as one of the languages they know how to code in.'"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["ask(\"Does he know how to code in Python?\", index, chunks)"]},{"cell_type":"code","execution_count":65,"metadata":{"cell_id":"faf85992c55e44dfa2b4219235078d61","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["'The person has worked on two projects based on the provided context information.\\n\\n1. A project related to analyzing work orders text using NLP and machine learning algorithms for categorizing failures. Additionally, they developed 10 PowerBI dashboards, optimized data models using DAX queries, and analyzed shift hours and badging data for peak hour optimization.\\n2. A GitHub project called \"githubClassGPT\" that involved Python, LlamaIndex, LangChain, AWS S3, OpenAI, Docker, and Streamlit. They developed a chatbot with Netlify and Cloud Run, improved search speed using concurrent processing in FastAPI, and built Svelte components to render search results in a user-friendly design.'"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["ask(\"What projects has he worked on?\", index, chunks)"]},{"cell_type":"code","execution_count":66,"metadata":{"cell_id":"d9fdaea143444d4e8d486b1466ecb4b4","deepnote_cell_type":"code"},"outputs":[{"data":{"text/plain":["'Based on the provided context information, it is not explicitly stated whether the individual is currently on the job market or not. The information only mentions their skills, experiences, and past leadership roles. To determine their employment status, additional context or information is needed.'"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["ask(\"Is he on the job market?\", index, chunks)"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"43f57242690d4d708c4a1ec839667b04","language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
